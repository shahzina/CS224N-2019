{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code practice from notes \n",
    "\n",
    "**NLTK Book** http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LazyCorpusLoader in module nltk.corpus.util object:\n",
      "\n",
      "wordnet = class LazyCorpusLoader(builtins.object)\n",
      " |  To see the API documentation for this lazily loaded corpus, first\n",
      " |  run corpus.ensure_loaded(), and then run help(this_corpus).\n",
      " |  \n",
      " |  LazyCorpusLoader is a proxy object which is used to stand in for a\n",
      " |  corpus object before the corpus is loaded.  This allows NLTK to\n",
      " |  create an object for each corpus, but defer the costs associated\n",
      " |  with loading those corpora until the first time that they're\n",
      " |  actually accessed.\n",
      " |  \n",
      " |  The first time this object is accessed in any way, it will load\n",
      " |  the corresponding corpus, and transform itself into that corpus\n",
      " |  (by modifying its own ``__class__`` and ``__dict__`` attributes).\n",
      " |  \n",
      " |  If the corpus can not be found, then accessing this object will\n",
      " |  raise an exception, displaying installation instructions for the\n",
      " |  NLTK data package.  Once they've properly installed the data\n",
      " |  package (or modified ``nltk.data.path`` to point to its location),\n",
      " |  they can then use the corpus object without restarting python.\n",
      " |  \n",
      " |  :param name: The name of the corpus\n",
      " |  :type name: str\n",
      " |  :param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n",
      " |  :type reader: nltk.corpus.reader.api.CorpusReader\n",
      " |  :param nltk_data_subdir: The subdirectory where the corpus is stored.\n",
      " |  :type nltk_data_subdir: str\n",
      " |  :param *args: Any other non-keywords arguments that `reader_cls` might need.\n",
      " |  :param *kargs: Any other keywords arguments that `reader_cls` might need.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      " |  \n",
      " |  __init__(self, name, reader_cls, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Wordnet Interface \n",
    "\n",
    "Link - https://www.nltk.org/howto/wordnet.html <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shahzinakhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up a word using synsets(); this function has an optional pos argument which lets you constrain the part of speech of the word. (from documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good,goodness\n",
      "noun: good,goodness\n",
      "noun: commodity,trade_good,good\n",
      "adj: good\n",
      "adj (s): full,good\n",
      "adj: good\n",
      "adj (s): estimable,good,honorable,respectable\n",
      "adj (s): beneficial,good\n",
      "adj (s): good\n",
      "adj (s): good,just,upright\n",
      "adj (s): adept,expert,good,practiced,proficient,skillful,skilful\n",
      "adj (s): good\n",
      "adj (s): dear,good,near\n",
      "adj (s): dependable,good,safe,secure\n",
      "adj (s): good,right,ripe\n",
      "adj (s): good,well\n",
      "adj (s): effective,good,in_effect,in_force\n",
      "adj (s): good\n",
      "adj (s): good,serious\n",
      "adj (s): good,sound\n",
      "adj (s): good,salutary\n",
      "adj (s): good,honest\n",
      "adj (s): good,undecomposed,unspoiled,unspoilt\n",
      "adj (s): good\n",
      "adv: well,good\n",
      "adv: thoroughly,soundly,good\n"
     ]
    }
   ],
   "source": [
    "poses = {'n' : 'noun',\n",
    "        'v' : 'verb',\n",
    "        's' : 'adj (s)',\n",
    "        'a' : 'adj',\n",
    "        'r' : 'adv'}\n",
    "\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()],\n",
    "                         \",\".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj: happy\n",
      "adj (s): felicitous,happy\n",
      "adj (s): glad,happy\n",
      "adj (s): happy,well-chosen\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets(\"happy\"):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()],\n",
    "                         \",\".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('happy.s.04.happy'), Lemma('happy.s.04.well-chosen')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('happy.s.04')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = wn.synset(\"panda.n.01\") #also tried \"sunflower.n.01\"\n",
    "hyper = lambda s: s.hypernyms()\n",
    "\n",
    "list(panda.closure(hyper))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Synset.closure at 0x1a1d5ff6d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda.closure(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using depth argument\n",
    "list(panda.closure(hyper, depth = 1)) #also tried depth = 5, prints 5 hypernyms as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides explaining similarity of vectors, <br>\n",
    "one hot encoded vectors, <br>\n",
    "why WordNet's synonyms should not be used for similarity. <br>\n",
    "\n",
    "Recommended : Code similarity in vectors themselves. <br>\n",
    "\n",
    "Important thought -> representing words by their context and not as discrete symbols (like One hot encodings) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
